---
title: "Project 2: Modeling"
output:
  pdf_document: default
  html_document: default
---
##*John Ryan Strubelt Jrs7346

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, message=FALSE, warning=FALSE, fig.width=8, tidy=TRUE)
```
 
##*Introduction*
I choose the following Data because it meet all the requirements for the project. There are 5 variables, there 118 observations.There is 2 Categorical Variables, 1 of which has 3 categories, and the other which has the potential to be binary. The 5 variables are Race in which the race can be Black, White or Hispanic. Then there is the position of the firefighter and that is Captain or Leut. and then the 3 categorical variables are Written score, Oral score, and Combined. However Combined is manipulated due to the fact that it incorperates 60% of the Written and 40% of the Oral.
```{r}
install.packages("Stat2Data")
library(Stat2Data)
library(dplyr)
library(tidyr)
data(Ricci)

```
##*MANOVA*
```{r}
library(ggplot2)
head(Ricci)
man1<-manova(cbind(Oral,Written)~Race,data=Ricci)
#Ho:For Oral score and Written score, means for each Race are equal
#Ha:Oral score or Written score, for alteast one Races mean differ
summary(man1)
#Significant, So since they are significant we reject the null meaning that there is some difference between the means. With that we are now going to do Univariate Anovas to get a better look at the variables.
summary(aov(Oral~Race,data = Ricci))
summary(aov(Written~Race,data = Ricci))
#There is significant difference between average oral exam scores between races. There is also a significant difference for the written. With that being said, a post hoc analysis will be ran to see which groups differ.
pairwise.t.test(Ricci$Oral,Ricci$Race,p.adj="none")
#There is a significant difference between oral Scores between indiviudals who are Black and Hispanic, and there is a significant difference between those who are Hispanic and White. But there is no significant difference between those who are Black and White. Meaning that the group who stands out are those that are Hispanic while looking at the oral exams
pairwise.t.test(Ricci$Written,Ricci$Race,p.adj="none")
#There is a significant difference between written scores for individuals who are balck and Hispanic, and those who are black and white, but there is no significant difference when looking at those who are hispanic and White. Meaning that there is significant difference in Written scores for people whos race is black.
#So far I ran 1 Manova, 2 Anovas, 6 T-test. I did 2 pair-wise T test which is equivalent to 6 due to the each one looking at 3 categories for a total of 9 test. 
#Since we conducted 6 hypothesis test (Post-hoc) the probablity of have at least one type 1 error is 0.26490810937 (1-.95^6)
#bonferoni Correction we will divide alpha which is .05 by 6 to get a new alpha of .0833
#The Assumptions for an Anova are Random Sample/Independent Observations, Independent Samples, Normal Distribution of each group, Equal Variance. I think one of the assumptions that would be broken would be the sample size of the Hispanic Race due to the fact that they have a smaller sample size the rest. With that the variance of the samples is most likely going to be affected since the White race has a value which is nearly trippled that
Ricci%>%count(Race)
```
##*Randomization Test*
```{r}

#Oral Score is the same for all 3 Races:Ho
#Oral Score is different between all 3 Races:Ha
#Very Confused on this part. Since my Categorical Variable has 3 categories rather than 2 I did the mean for Black Vs White, Black vs Hispanic, and for Hispanic vs White. The only example of the randomization test I saw was from lecture 14 and there was only two categories. In this case since there is 3 I looked at the means for all 3 added the means together than averaged the means of the means. 

rand_dist<-vector()
for(i in 1:5000){
  new<-data.frame(Oral=sample(Ricci$Oral),Race=Ricci$Race)
  rand_dist[i]<-((mean(new[new$Race=="B",]$Oral)-
    mean(new[new$Race=="H",]$Oral))+
    (mean(new[new$Race=="B",]$Oral)-   
    mean(new[new$Race=="W",]$Oral))+
    (mean(new[new$Race=="H",]$Oral)-
     mean(new[new$Race=="W",]$Oral)))
}
{hist(rand_dist,main = "",ylab = "");abline(v=-0.1293487,col="red")}
mean(rand_dist>-0.1293487)*2
#Pvalue is close to one so fail to reject the null hypopthesis. Meaning that the data is not significant.

```

```{r}
#linear regression
library(lmtest)
library(sandwich)
fit<-lm(Combine~Race+Written,data = Ricci)
summary(fit)
ggplot(Ricci,aes(x=Written, y=Combine, group=Race))+geom_point(aes(Color=Race))+geom_smooth(method = "lm",formula = y~1,fullrange=T,aes(color=Race))+theme(legend.position = c(.95,.15))+xlab("")
resids<-fit$residuals
fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept = 0,color='Red')#linear
ggplot()+geom_qq(aes(sample=resids))+geom_qq_line(aes(sample=resids))#this graph looks very off but the test ran below say that the normality is okay
ks.test(resids,"pnorm",mean=0,sd(resids))#P value is .8641 meaning we fail to reject the null which is that the distribution is normal 
shapiro.test(resids)#pvalue is .237 fail to reject the null that the distribution is normal
#it appears to meet all the assumptions but im going to recompute the regression regardless with robust standard errors
fit<-lm(Combine~Race+Written,data = Ricci)
bptest(fit)#Pvalue is non significant still going to continue
summary(fit)$coef[,1:2]
coeftest(fit,vcov=vcovHC(fit))[,1:2]
#when looking at the changes in the standard error, the changes are not that big they are changes by decimals only. 
#Just a reminder that the R2 is the proportion of variation in the response varible and that is .7859 the Adjusted R2 is .7801 and this value should be more conservative.
```
##*Bootstrapped Standard Error*
```{r}

boot_dat<-Ricci[sample(nrow(Ricci),replace = TRUE),]
 fit2<-lm(Combine~Race+Written,data = boot_dat)
samp_distn<-replicate(5000,{
  boot_dat<-Ricci[sample(nrow(Ricci),replace = TRUE),]
  fit2<-lm(Combine~Race+Written,data = boot_dat)
  coef(fit2)
})
summary(fit2)
samp_distn%>%t%>%as.data.frame%>%summarise_all(sd)#Est. Se
samp_distn%>%t%>%as.data.frame%>%gather%>%group_by(key)%>%summarize(lower=quantile(value,.025),upper=quantile(value,.975))
#while compairng the pvalues from this section to the pvlaues from above, the same pvalues that were significant above are still significant in this case. In addition to that the only one that isnt significnat is Being White. While looking at the SE, they seem to have gotten smaller and smaller with each test that we have performed on them.
```

##*Logistic Regression*
```{r}

library(plotROC)
Ricci$Position<-ifelse(Ricci$Position=="Captain",1,0)
fit3<-glm(Position~Race+Oral+Written,data = Ricci,family = binomial(link = "logit"))
coeftest(fit3)
#Oral is significant meaning that it increases the log-odds of Position. So i am assuming the better verbal skills these people have makes them more likely to be in a certain position.
probs<-predict(fit3,type="response")
mean(probs>.99)
class_diag<-function(probs,truth){
 tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
 acc=sum(diag(tab))/sum(tab)
 sens=tab[2,2]/colSums(tab)[2]
 spec=tab[1,1]/colSums(tab)[1]
 ppv=tab[2,2]/rowSums(tab)[2]
 if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
 ord<-order(probs, decreasing=TRUE)
 probs <- probs[ord]; truth <- truth[ord]
 TPR=cumsum(truth)/max(1,sum(truth))
 FPR=cumsum(!truth)/max(1,sum(!truth))
 dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
 TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
 n <- length(TPR)
 auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
 data.frame(acc,sens,spec,ppv,auc)
} 
class_diag(probs,Ricci$Position)
table(predict=as.numeric(probs>.5),truth=Ricci$Position)%>%addmargins
8/41#TPR/Sens
69/77#TNR/Spec
(69+8)/118#Accuracy
#With the AUC being .66 this makes this a really bad fit. Meaning that currently with the current fit we can be misclasssifying things. The true positive rate is .19 the true negative rate is .89 and accuracy is .65 making this a poor model
ROCplot<-ggplot(Ricci)+geom_roc(aes(d=Ricci$Position,m=predict(fit3,type="response")),n.cuts=0)
ROCplot
calc_auc(ROCplot)
#The AUC matches with that from above. As stated above, due to the AUC being what it is, it makes it a poor represenation. Also the way the graph s, it means that the it is hard to predict Position based off of the values that we decided to use.


```
##*10-fold*
```{r}

set.seed(1234)
k=10

data1<-Ricci[sample(nrow(Ricci)),]
folds<-cut(seq(1:nrow(Ricci)),breaks=k,labels=FALSE) 
diags<-NULL

for(i in 1:k){
 train<-data1[folds!=i,]
 test<-data1[folds==i,]
 truth2<-test$Position

 fit4<-glm(Position~Race+Oral+Written,data=train,family="binomial")
 probs2<-predict(fit4,newdata = test,type="response")
 diags<-rbind(diags,class_diag(probs2,truth2))
}
apply(diags,2,mean)
#In this step, one thing that was noted was that there is change in Accuarcy and TPR they both decrease meaning that this new model still is not the best fit. In addition to that the AUC also got smaller meaning that these predictors are still not a great way of predicting Postion.

```
##*LASSO*
```{r}

fit5<-lm(Combine~.,data=Ricci)
summary(fit5)

library(glmnet)
y<-as.matrix(Ricci$Position)
x<-model.matrix(fit5)[,-1]
x<-scale(x)
cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family = "binomial", lambda = cv$lambda.1se)
coef(lasso)
#position is the only one with a value next to it so it will be used in the 10 fold CV for the next part.
set.seed(1234)
data2<-Ricci[sample(nrow(Ricci)),]
folds<-cut(seq(1:nrow(Ricci)),breaks = k,labels = FALSE)

diags<-NULL
for(i in 1:k){
  train2<-data2[folds!=i,]
  test2<-data2[folds==i,]
  truth3<-test2$Combine
  
  fit6<-lm(Combine~Position,data=train2)
  probs4<-predict(fit6,newdata = test2,type = "response")
  preds<-ifelse(probs4>.5,1,0)
  
  diags<-rbind(diags,class_diag(probs4,truth3))
}
diags%>%summarize_all(mean)
summary(fit5)
summary(fit6)
#Since the response is going to be a Numeric value, I am going to use the Residual Standard Error from fit5 to compare it to my lasso fit which is fit 6. In fit 5 I wanted to see if the Combine score was predicted well by all the other variables. Once performing a Lasso the only varible that came back was position. A 10 fold CV was then performed on the Combined Score only looking at postion. The residual standard error came back as 9.102 which was greater than that Residual standard error from fit 5 which was  4.084e-15. Since the value was higher, it is not a better fit. 
```

